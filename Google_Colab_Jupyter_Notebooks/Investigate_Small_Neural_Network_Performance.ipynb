{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Investigate_Small_Neural_Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-learning-airu-ozone/ChE_Utah_AirQuality_ML_Ozone/blob/master/Google_Colab_Jupyter_Notebooks/Investigate_Small_Neural_Network_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYRyGd5toEtA",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://)\n",
        "# Investigate Small Neural Network\n",
        "## Summer 2019-Air U Project \n",
        "### Timothy Quah\n",
        "\n",
        "This script was used to investigate the minimal number of nodes and layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrWNnKfpo7jE",
        "colab_type": "text"
      },
      "source": [
        "###  Load Data into Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "521dQaTBoaXF",
        "colab_type": "text"
      },
      "source": [
        "First thing we must do is to load our data into the Google Colab environment. To do this we must do the following:\n",
        "First we need to mount the drive which we use the following lines to do. There are ways to automate this process, but I am honestly a bit too lazy to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQkUBhbzoBJg",
        "colab_type": "code",
        "outputId": "9f28d956-1b8c-4440-db0f-a7b57ee411bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s6yBjx-pt8N",
        "colab_type": "text"
      },
      "source": [
        "The next step we will do is to verify that the path exist to our files that we have stored on google drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggZr0_QMxNId",
        "colab_type": "code",
        "outputId": "2e0e2329-9e5d-44d1-ff61-c3e04fca6901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import_data_path = '/content/drive/My Drive/AirQuality_Research/Remove_Params_Upload_GoogleDrive/SWD/'\n",
        "\n",
        "\n",
        "print('Does Data Path? '+str(os.path.exists(import_data_path)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Does Data Path? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEYQf1XBABor",
        "colab_type": "text"
      },
      "source": [
        "Next we need to clone our repository and then load it into this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP8EN2C4AJIY",
        "colab_type": "code",
        "outputId": "203e03a2-8f5a-42e3-fd39-0cd86be524d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Repository_Path = '/content/drive/My Drive/AirQuality_Research/Repository'\n",
        "print('Does Repo Path? '+str(os.path.exists(Repository_Path)))\n",
        "os.chdir(Repository_Path)\n",
        "\n",
        "\n",
        "if len(os.listdir('/content/drive/My Drive/AirQuality_Research/Repository'))>0:\n",
        "  !rm -r *\n",
        "  print('Old Repository Deleted')\n",
        "  \n",
        "! git clone https://github.com/machine-learning-airu-ozone/ChE_Utah_AirQuality_ML_Ozone\n",
        "import_script_path = '/content/drive/My Drive/AirQuality_Research/Repository/ChE_Utah_AirQuality_ML_Ozone/Functions'\n",
        "print('Does Function Path? '+str(os.path.exists(import_script_path)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Does Repo Path? True\n",
            "Old Repository Deleted\n",
            "Cloning into 'ChE_Utah_AirQuality_ML_Ozone'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 40 (delta 8), reused 28 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), done.\n",
            "ChE_Utah_AirQuality_ML_Ozone\n",
            "Does Function Path? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WcmQzHVyWtA",
        "colab_type": "text"
      },
      "source": [
        "Next we import all the packages we need. If you need a specific version it is possible, but will take an extra line: example  !pip install seaborn==0.9.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbNnV-aAyaZj",
        "colab_type": "code",
        "outputId": "f1ac2b24-3257-43f8-830d-edc964f9e1aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.optimizers as optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from keras import backend as K\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7F7XMIn1La2",
        "colab_type": "text"
      },
      "source": [
        "Now we need to load some scripts from our custom made functions into the enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL6eqJWo1TvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(import_script_path)\n",
        "from Trainer_Functions import r2_keras,model_neural_network,load_evaluate_neural_net,norm_divider,divider_XY,mse,r2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwebNnvN4neZ",
        "colab_type": "text"
      },
      "source": [
        "This function is used to generate the exported textfile "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CYHo5XykMQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def export_text(data,name_list,layers,repeat):\n",
        "    name = 'layers_'+str(layers)+'_low_node_repeat_'+str(repeat)+'.txt'\n",
        "    text_file = open(name,'w')\n",
        "    for i in name_list:\n",
        "        text_file.write(i)\n",
        "        text_file.write(' ')\n",
        "        \n",
        "    row,column = data.shape\n",
        "    for i in range(0,row,1):\n",
        "        text_file.write('\\n')\n",
        "        for j in range(0,column,1):\n",
        "            text_file.write(str(data[i,j]))\n",
        "            text_file.write(' ')\n",
        "    text_file.close()\n",
        "    return name\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIWzCApazPyV",
        "colab_type": "text"
      },
      "source": [
        "Next we need to just make sure to close all plots and just in case we have set random seeds\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJt6W9_m2Mqq",
        "colab_type": "code",
        "outputId": "4856f90f-cf3b-4037-d7e1-62f0084916d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "plt.close('all')\n",
        "load_data_list = os.listdir(import_data_path)\n",
        "print(load_data_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['outsider_data.csv', 'train_validate_data.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKehy-FG3L89",
        "colab_type": "text"
      },
      "source": [
        "We see in this case we have two files one called 'outsider_data.csv' we will load both files (including 'train_validate_data.csv') now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqAlFzlF2_N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "full_path = os.path.join(import_data_path,load_data_list[i])\n",
        "df = pd.read_csv(full_path)\n",
        "header =list(df) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOYxsDx15ZCw",
        "colab_type": "code",
        "outputId": "d98d2fcd-c689-4a02-9b2b-dc9744b53955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(header)\n",
        "print(len(header))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Day', 'Hour', 'Month', 'CO_AIR_U_Sensor', 'Humidity_AIR_U_Sensor', 'MICS_AIR_U_Sensor', 'NO_AIR_U_Sensor', 'O3 Value', 'PM10_AIR_U_Sensor', 'PM1_AIR_U_Sensor', 'RH Value', 'SR Value', 'SWS Value', 'TEMP Value', 'Temperature_AIR_U_Sensor', 'Min_CO_AIR_U_Sensor', 'Max_CO_AIR_U_Sensor', 'Min_Humidity_AIR_U_Sensor', 'Max_Humidity_AIR_U_Sensor', 'Min_MICS_AIR_U_Sensor', 'Max_MICS_AIR_U_Sensor', 'Min_NO_AIR_U_Sensor', 'Max_NO_AIR_U_Sensor', 'Min_PM10_AIR_U_Sensor', 'Max_PM10_AIR_U_Sensor', 'Min_PM1_AIR_U_Sensor', 'Max_PM1_AIR_U_Sensor', 'Min_Temperature_AIR_U_Sensor', 'Max_Temperature_AIR_U_Sensor']\n",
            "29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FDEN1zQ5lq8",
        "colab_type": "text"
      },
      "source": [
        "Now we need to divide the training data from the validation data as well as inputs from outputs. In this case we will set up 70% training and 30% Validation.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9IrtAqbwo5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "na_len = 20\n",
        "nodes_array = np.linspace(6,150,na_len,dtype=int)\n",
        "layer = 4\n",
        "learning_rate = 0.05\n",
        "epochs = 20\n",
        "batchsize = 32\n",
        "droprate = 0.2\n",
        "repeat = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUg0CuM152yC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header_num = len(header)\n",
        "Full_List = list(np.arange(0,header_num-1+1e-6,1,dtype=int))\n",
        "Y_Loc =  header.index('O3 Value')\n",
        "Y_header_list = []\n",
        "Y_header_list.append(Y_Loc)\n",
        "X_header_list = list(set(Full_List)-set(Y_header_list))\n",
        "data_array = np.array(df)\n",
        "train_list,valid_list = norm_divider(data_array)\n",
        "X,Y,X_valid,Y_valid = divider_XY(X_header_list,Y_header_list,data_array,train_list,valid_list)\n",
        "mse_mean_array = np.zeros(na_len)\n",
        "r2_mean_array = np.zeros(na_len)\n",
        "mse_std_array = np.zeros(na_len)\n",
        "r2_std_array = np.zeros(na_len)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ2iFnOQ0Jn9",
        "colab_type": "text"
      },
      "source": [
        "Now we need to specify the neural network archetecture (layers (how deep) and nodes (how wide)), the learning rate (how aggressive the optimizer is),  how long to train, and batch sizes. Finally we want to use the validation data to evaluate the neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOZ2GHkl0nYT",
        "colab_type": "code",
        "outputId": "821b02d8-d673-4301-b73e-5fe122ce7435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "for i in range(0,na_len,1):\n",
        "  print('i = %d'%i)\n",
        "  mse_temp = np.zeros(repeat)\n",
        "  r2_temp = np.zeros(repeat)\n",
        "  for j in range(0,repeat,1):\n",
        "    K.clear_session()\n",
        "    nodes = nodes_array[i]\n",
        "\n",
        "    input_dim_ = len(X_header_list)\n",
        "    output_dim_ = len(Y_header_list)\n",
        "    model = model_neural_network(layer,nodes,input_dim_,output_dim_,DropPercent = droprate)\n",
        "    optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(loss='mean_squared_error',optimizer=\"adam\", metrics=[r2_keras])\n",
        "    history = model.fit(x=X, y = Y, nb_epoch=epochs, batch_size=batchsize,verbose = 0)\n",
        "    Y_pred = model.predict(X_valid)\n",
        "    mse_temp[j] = mse(Y_valid,Y_pred)\n",
        "    r2_temp[j] = r2(Y_valid,Y_pred)\n",
        "    print('j = %d'%j)\n",
        "  mse_mean_array[i] = np.mean(mse_temp)\n",
        "  r2_mean_array[i] = np.mean(r2_temp)\n",
        "  mse_std_array[i] = np.std(mse_temp)\n",
        "  r2_std_array[i] = np.std(r2_temp)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0816 01:16:58.153837 140198587930496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "W0816 01:16:58.155252 140198587930496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0816 01:16:58.173322 140198587930496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0816 01:16:58.175175 140198587930496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0816 01:16:58.181718 140198587930496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0816 01:16:58.200640 140198587930496 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0816 01:16:58.318188 140198587930496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d4f0b148b445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr2_keras\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmse_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZOLFz11Qw0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.errorbar(nodes_array, mse_mean_array, mse_std_array,fmt='o')\n",
        "plt.xlabel('Nodes (n)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "\n",
        "name = str(layer)+'_MSE_Nodes_Study.jpg'\n",
        "plt.savefig(name,dpi=300)\n",
        "files.download(name)\n",
        "\n",
        "plt.figure()\n",
        "plt.errorbar(nodes_array, r2_mean_array, r2_std_array,fmt='o')\n",
        "plt.xlabel('Nodes (n)')\n",
        "plt.ylabel('Linearity ($R^2$)')\n",
        "\n",
        "name = str(layer)+'_R2_Nodes_Study.jpg'\n",
        "plt.savefig(name,dpi=300)\n",
        "files.download(name)\n",
        "\n",
        "print(nodes_array)\n",
        "print(mse_mean_array)\n",
        "print(r2_mean_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGk6PqrLkHeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.vstack([nodes_array,mse_mean_array,mse_std_array,r2_mean_array,r2_std_array]).transpose()\n",
        "name_list = ['Nodes','MSE_Mean','MSE_StD','R2_Mean','R2_StD']\n",
        "name =export_text(data,name_list,layer,repeat)\n",
        "files.download(name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
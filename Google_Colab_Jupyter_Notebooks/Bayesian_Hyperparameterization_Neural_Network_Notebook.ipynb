{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian_Hyperparameterization_Neural_Network_Notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-learning-airu-ozone/ChE_Utah_AirQuality_ML_Ozone/blob/master/Bayesian_Hyperparameterization_Neural_Network_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYRyGd5toEtA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Neural Network Notebook Optimization-Bayesian Template\n",
        "## Summer 2019-Air U Project \n",
        "### Timothy Quah\n",
        "\n",
        "This template is an example of how to train neural networks on Google Colab. Google Colab has free GPU/TPU resources that can help train neural networks that deal with large datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrWNnKfpo7jE",
        "colab_type": "text"
      },
      "source": [
        "###  Load Data into Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "521dQaTBoaXF",
        "colab_type": "text"
      },
      "source": [
        "First thing we must do is to load our data into the Google Colab environment. To do this we must do the following:\n",
        "First we need to mount the drive which we use the following lines to do. There are ways to automate this process, but I am honestly a bit too lazy to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQkUBhbzoBJg",
        "colab_type": "code",
        "outputId": "24010b37-93f3-4f71-abd9-ed22b67bb5b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s6yBjx-pt8N",
        "colab_type": "text"
      },
      "source": [
        "The next step we will do is to verify that the path exist to our files that we have stored on google drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggZr0_QMxNId",
        "colab_type": "code",
        "outputId": "2b0f554d-a0d9-426f-beb3-98d84e280466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import os\n",
        "import_data_path = '/content/drive/My Drive/AirQuality_Research/Remove_Params_Upload_GoogleDrive/SWD/'\n",
        "\n",
        "\n",
        "print('Does Data Path? '+str(os.path.exists(import_data_path)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Does Data Path? True\n",
            "Does Function Path? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEYQf1XBABor",
        "colab_type": "text"
      },
      "source": [
        "Next we need to clone our repository and then load it into this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP8EN2C4AJIY",
        "colab_type": "code",
        "outputId": "203e03a2-8f5a-42e3-fd39-0cd86be524d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Repository_Path = '/content/drive/My Drive/AirQuality_Research/Repository'\n",
        "print('Does Repo Path? '+str(os.path.exists(Repository_Path)))\n",
        "os.chdir(Repository_Path)\n",
        "\n",
        "\n",
        "if len(os.listdir('/content/drive/My Drive/AirQuality_Research/Repository'))>0:\n",
        "  !rm -r *\n",
        "  print('Old Repository Deleted')\n",
        "  \n",
        "! git clone https://github.com/machine-learning-airu-ozone/ChE_Utah_AirQuality_ML_Ozone\n",
        "import_script_path = '/content/drive/My Drive/AirQuality_Research/Repository/ChE_Utah_AirQuality_ML_Ozone/Functions'\n",
        "print('Does Function Path? '+str(os.path.exists(import_script_path)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Does Repo Path? True\n",
            "Old Repository Deleted\n",
            "Cloning into 'ChE_Utah_AirQuality_ML_Ozone'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 40 (delta 8), reused 28 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), done.\n",
            "ChE_Utah_AirQuality_ML_Ozone\n",
            "Does Function Path? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WcmQzHVyWtA",
        "colab_type": "text"
      },
      "source": [
        "Next we import all the packages we need. If you need a specific version it is possible, but will take an extra line: example  !pip install seaborn==0.9.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbNnV-aAyaZj",
        "colab_type": "code",
        "outputId": "00650fae-c623-4259-b25e-0623696c9f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.optimizers as optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from keras import backend as K\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from functools import partial\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r77kq9fZDpSM",
        "colab_type": "text"
      },
      "source": [
        "We need to install bayesian optimization we do this using the following method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypn3Ju4JDudN",
        "colab_type": "code",
        "outputId": "12f22167-02f6-4343-b8e0-f01db90f5093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.21.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.13.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7F7XMIn1La2",
        "colab_type": "text"
      },
      "source": [
        "Now we need to load some scripts from our custom made functions into the enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL6eqJWo1TvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(import_script_path)\n",
        "from Trainer_Functions import r2_keras,model_neural_network,load_evaluate_neural_net,norm_divider,divider_XY,mse,r2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIWzCApazPyV",
        "colab_type": "text"
      },
      "source": [
        "Next we need to just make sure to close all plots and just in case we have set random seeds\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJt6W9_m2Mqq",
        "colab_type": "code",
        "outputId": "9048c968-3faa-44df-8af7-b5b5428e6e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "plt.close('all')\n",
        "random.seed(7)\n",
        "np.random.seed(7)\n",
        "load_data_list = os.listdir(import_data_path)\n",
        "print(load_data_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['outsider_data.csv', 'train_validate_data.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKehy-FG3L89",
        "colab_type": "text"
      },
      "source": [
        "We see in this case we have two files one called 'All_Data_norm.csv' we will load both files (including 'train_validate_data.csv') now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqAlFzlF2_N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "full_path = os.path.join(import_data_path,load_data_list[i])\n",
        "df = pd.read_csv(full_path)\n",
        "header =list(df) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FDEN1zQ5lq8",
        "colab_type": "text"
      },
      "source": [
        "Now we need to divide the training data from the validation data as well as inputs from outputs. In this case we will set up 70% training and 30% Validation.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUg0CuM152yC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header_num = len(header)\n",
        "Full_List = list(np.arange(0,header_num-1+1e-6,1,dtype=int))\n",
        "Y_Loc =  header.index('O3 Value')\n",
        "Y_header_list = []\n",
        "Y_header_list.append(Y_Loc)\n",
        "X_header_list = list(set(Full_List)-set(Y_header_list))\n",
        "\n",
        "data_array = np.array(df)\n",
        "train_list,valid_list = norm_divider(data_array)\n",
        "X,Y,X_valid,Y_valid = divider_XY(X_header_list,Y_header_list,data_array,train_list,valid_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ2iFnOQ0Jn9",
        "colab_type": "text"
      },
      "source": [
        "Now we need to prepare the data in the other dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOZ2GHkl0nYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 0\n",
        "full_path = os.path.join(import_data_path,load_data_list[i])\n",
        "df_test = pd.read_csv(full_path)\n",
        "\n",
        "other_header = list(df_test)\n",
        "for i in range(0,len(delete_list)):\n",
        "  index = other_header.index(delete_list[i])\n",
        "  del df_test[other_header[index]]\n",
        "  del other_header[index]\n",
        "\n",
        "data_array_test = np.array(df_test)\n",
        "X_Other = data_array_test[:,X_header_list]\n",
        "Y_Other = data_array_test[:,Y_header_list]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXDfVFhO4BWU",
        "colab_type": "text"
      },
      "source": [
        "We now need to create an optimization function pay careful attention to datatypes and that we are maximizing not minimizing thus we take the negative of error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS9nXQYx4NLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def all_optimize_function(X,X_valid,X_other,X_header_list,\\\n",
        "                         Y,Y_valid,Y_other,Y_header_list,\\\n",
        "                         errorweight,return_model,verbose_set,\\\n",
        "                         layers,nodes,droprate,\\\n",
        "                         learning_rate,batchsize,epoch):\n",
        "    layers = int(layers)\n",
        "    nodes = int(nodes)\n",
        "    batchsize = int(batchsize)\n",
        "    epoch = int(epoch)\n",
        "    input_dim_ = len(X_header_list)\n",
        "    output_dim_ = len(Y_header_list)\n",
        "    model = model_neural_network(layers,nodes,input_dim_,output_dim_,\\\n",
        "                                 DropPercent = droprate)\n",
        "    optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(loss='mean_squared_error',optimizer=\"adam\", metrics=[r2_keras])\n",
        "    history = model.fit(x=X, y = Y, nb_epoch=epoch, batch_size=batchsize,\\\n",
        "                        verbose=verbose_set)\n",
        "    Y_pred = model.predict(X_valid)\n",
        "    Valid_MSE = mse(Y_valid,Y_pred)\n",
        "    Y_other_pred = model.predict(X_other)\n",
        "    Other_MSE = mse(Y_other,Y_other_pred)\n",
        "    if return_model:\n",
        "        return -Other_MSE,model,history\n",
        "    else:\n",
        "        return -Other_MSE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFUQ-HfgUnJ8",
        "colab_type": "text"
      },
      "source": [
        "Now we need to set bounds on the parameters we are optimizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnKbj8ssUkLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimize_function = partial(all_optimize_function,X,X_valid,X_Other,X_header_list,\\\n",
        "                          Y,Y_valid,Y_Other,Y_header_list,errorweight = 1/3,\\\n",
        "                          return_model=False,verbose_set = 0)\n",
        "\n",
        "pbounds = {'layers':(1,5),'nodes':(10,50),'droprate':(0.01,0.2),\\\n",
        "       'learning_rate':(0.01,0.5),'batchsize':(20,80),'epoch':(50,100)}\n",
        "\n",
        "optimizer = BayesianOptimization(f=optimize_function,pbounds=pbounds,random_state=1,)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLhs_UU9Uti5",
        "colab_type": "text"
      },
      "source": [
        "Now lets do the optimization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lRmW0K7UwL8",
        "colab_type": "code",
        "outputId": "dad078a2-1047-41a0-bdd5-1722f20af271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        }
      },
      "source": [
        "points = 3\n",
        "itterations = 5\n",
        "optimizer.maximize(init_points=points,n_iter=itterations,)\n",
        "print(optimizer.max)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0725 19:53:14.335430 140177552824192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0725 19:53:14.353483 140177552824192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0725 19:53:14.356034 140177552824192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | batchsize | droprate  |   epoch   |  layers   | learni... |   nodes   |\n",
            "-------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0725 19:53:14.374746 140177552824192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0725 19:53:14.384134 140177552824192 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0725 19:53:14.530088 140177552824192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "W0725 19:53:14.808858 140177552824192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.0204  \u001b[0m | \u001b[0m 45.02   \u001b[0m | \u001b[0m 0.1469  \u001b[0m | \u001b[0m 50.01   \u001b[0m | \u001b[0m 5.116   \u001b[0m | \u001b[0m 0.08191 \u001b[0m | \u001b[0m 36.46   \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.01802 \u001b[0m | \u001b[95m 31.18   \u001b[0m | \u001b[95m 0.07566 \u001b[0m | \u001b[95m 69.84   \u001b[0m | \u001b[95m 6.772   \u001b[0m | \u001b[95m 0.2154  \u001b[0m | \u001b[95m 77.97   \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.0198  \u001b[0m | \u001b[0m 32.27   \u001b[0m | \u001b[0m 0.1768  \u001b[0m | \u001b[0m 51.37   \u001b[0m | \u001b[0m 7.693   \u001b[0m | \u001b[0m 0.2145  \u001b[0m | \u001b[0m 69.11   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[95m 4       \u001b[0m | \u001b[95m-0.01407 \u001b[0m | \u001b[95m 79.78   \u001b[0m | \u001b[95m 0.08362 \u001b[0m | \u001b[95m 98.94   \u001b[0m | \u001b[95m 9.886   \u001b[0m | \u001b[95m 0.4814  \u001b[0m | \u001b[95m 97.25   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.0191  \u001b[0m | \u001b[0m 78.4    \u001b[0m | \u001b[0m 0.1007  \u001b[0m | \u001b[0m 50.48   \u001b[0m | \u001b[0m 3.25    \u001b[0m | \u001b[0m 0.08316 \u001b[0m | \u001b[0m 99.78   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.01906 \u001b[0m | \u001b[0m 79.29   \u001b[0m | \u001b[0m 0.08562 \u001b[0m | \u001b[0m 99.81   \u001b[0m | \u001b[0m 7.37    \u001b[0m | \u001b[0m 0.3285  \u001b[0m | \u001b[0m 30.78   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[95m 7       \u001b[0m | \u001b[95m-0.01388 \u001b[0m | \u001b[95m 22.05   \u001b[0m | \u001b[95m 0.1198  \u001b[0m | \u001b[95m 98.65   \u001b[0m | \u001b[95m 3.791   \u001b[0m | \u001b[95m 0.2024  \u001b[0m | \u001b[95m 99.94   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[95m 8       \u001b[0m | \u001b[95m-0.01373 \u001b[0m | \u001b[95m 20.24   \u001b[0m | \u001b[95m 0.02814 \u001b[0m | \u001b[95m 99.93   \u001b[0m | \u001b[95m 6.29    \u001b[0m | \u001b[95m 0.08273 \u001b[0m | \u001b[95m 98.69   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.01488 \u001b[0m | \u001b[0m 24.23   \u001b[0m | \u001b[0m 0.1752  \u001b[0m | \u001b[0m 99.85   \u001b[0m | \u001b[0m 9.906   \u001b[0m | \u001b[0m 0.3497  \u001b[0m | \u001b[0m 99.14   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.01524 \u001b[0m | \u001b[0m 23.01   \u001b[0m | \u001b[0m 0.06055 \u001b[0m | \u001b[0m 99.08   \u001b[0m | \u001b[0m 3.032   \u001b[0m | \u001b[0m 0.1199  \u001b[0m | \u001b[0m 99.51   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.01726 \u001b[0m | \u001b[0m 21.38   \u001b[0m | \u001b[0m 0.1824  \u001b[0m | \u001b[0m 99.4    \u001b[0m | \u001b[0m 6.941   \u001b[0m | \u001b[0m 0.4391  \u001b[0m | \u001b[0m 99.55   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.01649 \u001b[0m | \u001b[0m 78.1    \u001b[0m | \u001b[0m 0.08109 \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 9.123   \u001b[0m | \u001b[0m 0.1315  \u001b[0m | \u001b[0m 97.85   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.0146  \u001b[0m | \u001b[0m 21.0    \u001b[0m | \u001b[0m 0.01365 \u001b[0m | \u001b[0m 99.32   \u001b[0m | \u001b[0m 9.564   \u001b[0m | \u001b[0m 0.01317 \u001b[0m | \u001b[0m 98.34   \u001b[0m |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
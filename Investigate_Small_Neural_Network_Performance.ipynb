{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Investigate_Small_Neural_Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-learning-airu-ozone/ChE_Utah_AirQuality_ML_Ozone/blob/master/Investigate_Small_Neural_Network_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYRyGd5toEtA",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://)\n",
        "# Investigate Small Neural Network\n",
        "## Summer 2019-Air U Project \n",
        "### Timothy Quah\n",
        "\n",
        "This script was used to investigate the minimal number of nodes and layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrWNnKfpo7jE",
        "colab_type": "text"
      },
      "source": [
        "###  Load Data into Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "521dQaTBoaXF",
        "colab_type": "text"
      },
      "source": [
        "First thing we must do is to load our data into the Google Colab environment. To do this we must do the following:\n",
        "First we need to mount the drive which we use the following lines to do. There are ways to automate this process, but I am honestly a bit too lazy to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQkUBhbzoBJg",
        "colab_type": "code",
        "outputId": "361ef26c-eca9-4e66-fa54-3cff704f90e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s6yBjx-pt8N",
        "colab_type": "text"
      },
      "source": [
        "The next step we will do is to verify that the path exist to our files that we have stored on google drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggZr0_QMxNId",
        "colab_type": "code",
        "outputId": "a32d3593-d76d-4807-8a49-9202c8a82f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "# import data\n",
        "import_data_path = '/content/drive/My Drive/AirQuality_Research/Remove_Params_Upload_GoogleDrive/SWD/'\n",
        "import_script_path = '/content/drive/My Drive/AirQuality_Research/Functions/'\n",
        "\n",
        "print('Does Data Path? '+str(os.path.exists(import_data_path)))\n",
        "print('Does Function Path? '+str(os.path.exists(import_script_path)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Does Data Path? True\n",
            "Does Function Path? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WcmQzHVyWtA",
        "colab_type": "text"
      },
      "source": [
        "Next we import all the packages we need. If you need a specific version it is possible, but will take an extra line: example  !pip install seaborn==0.9.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbNnV-aAyaZj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad18a928-b0f1-496a-a62f-2221e233a462"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.optimizers as optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from keras import backend as K\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7F7XMIn1La2",
        "colab_type": "text"
      },
      "source": [
        "Now we need to load some scripts from our custom made functions into the enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL6eqJWo1TvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(import_script_path)\n",
        "from Trainer_Functions import r2_keras,model_neural_network,load_evaluate_neural_net,norm_divider,divider_XY,mse,r2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwebNnvN4neZ",
        "colab_type": "text"
      },
      "source": [
        "This function is used to generate the exported textfile "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CYHo5XykMQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def export_text(data,name_list,layers,repeat):\n",
        "    name = 'layers_'+str(layers)+'_low_node_repeat_'+str(repeat)+'.txt'\n",
        "    text_file = open(name,'w')\n",
        "    for i in name_list:\n",
        "        text_file.write(i)\n",
        "        text_file.write(' ')\n",
        "        \n",
        "    row,column = data.shape\n",
        "    for i in range(0,row,1):\n",
        "        text_file.write('\\n')\n",
        "        for j in range(0,column,1):\n",
        "            text_file.write(str(data[i,j]))\n",
        "            text_file.write(' ')\n",
        "    text_file.close()\n",
        "    return name\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIWzCApazPyV",
        "colab_type": "text"
      },
      "source": [
        "Next we need to just make sure to close all plots and just in case we have set random seeds\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJt6W9_m2Mqq",
        "colab_type": "code",
        "outputId": "7b1384c7-f017-4a51-b608-a70f305fee02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "plt.close('all')\n",
        "load_data_list = os.listdir(import_data_path)\n",
        "print(load_data_list)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['outsider_data.csv', 'train_validate_data.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKehy-FG3L89",
        "colab_type": "text"
      },
      "source": [
        "We see in this case we have two files one called 'All_Data_norm.csv' we will load both files (including 'train_validate_data.csv') now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqAlFzlF2_N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "full_path = os.path.join(import_data_path,load_data_list[i])\n",
        "df = pd.read_csv(full_path)\n",
        "header =list(df) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6auTiOA5Z0Z",
        "colab_type": "text"
      },
      "source": [
        "Now we need to decide which parameters we want to include in the Neural Network. For the sake of the template/tutorial we will keep everything with the exception of  NO2/CO value from DAQ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOYxsDx15ZCw",
        "colab_type": "code",
        "outputId": "50c24539-d458-42fb-c943-f00657c1a5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(header)\n",
        "print(len(header))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Day', 'Hour', 'Month', 'CO_AIR_U_Sensor', 'Humidity_AIR_U_Sensor', 'MICS_AIR_U_Sensor', 'NO_AIR_U_Sensor', 'O3 Value', 'PM10_AIR_U_Sensor', 'PM1_AIR_U_Sensor', 'RH Value', 'SR Value', 'SWS Value', 'TEMP Value', 'Temperature_AIR_U_Sensor', 'Min_CO_AIR_U_Sensor', 'Max_CO_AIR_U_Sensor', 'Min_Humidity_AIR_U_Sensor', 'Max_Humidity_AIR_U_Sensor', 'Min_MICS_AIR_U_Sensor', 'Max_MICS_AIR_U_Sensor', 'Min_NO_AIR_U_Sensor', 'Max_NO_AIR_U_Sensor', 'Min_PM10_AIR_U_Sensor', 'Max_PM10_AIR_U_Sensor', 'Min_PM1_AIR_U_Sensor', 'Max_PM1_AIR_U_Sensor', 'Min_Temperature_AIR_U_Sensor', 'Max_Temperature_AIR_U_Sensor']\n",
            "29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FDEN1zQ5lq8",
        "colab_type": "text"
      },
      "source": [
        "Now we need to divide the training data from the validation data as well as inputs from outputs. In this case we will set up 70% training and 30% Validation.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9IrtAqbwo5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "na_len = 20\n",
        "nodes_array = np.linspace(6,150,na_len,dtype=int)\n",
        "layer = 4\n",
        "learning_rate = 0.05\n",
        "epochs = 20\n",
        "batchsize = 32\n",
        "droprate = 0.2\n",
        "repeat = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUg0CuM152yC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header_num = len(header)\n",
        "Full_List = list(np.arange(0,header_num-1+1e-6,1,dtype=int))\n",
        "Y_Loc =  header.index('O3 Value')\n",
        "Y_header_list = []\n",
        "Y_header_list.append(Y_Loc)\n",
        "X_header_list = list(set(Full_List)-set(Y_header_list))\n",
        "data_array = np.array(df)\n",
        "train_list,valid_list = norm_divider(data_array)\n",
        "X,Y,X_valid,Y_valid = divider_XY(X_header_list,Y_header_list,data_array,train_list,valid_list)\n",
        "mse_mean_array = np.zeros(na_len)\n",
        "r2_mean_array = np.zeros(na_len)\n",
        "mse_std_array = np.zeros(na_len)\n",
        "r2_std_array = np.zeros(na_len)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ2iFnOQ0Jn9",
        "colab_type": "text"
      },
      "source": [
        "Now we need to specify the neural network archetecture (layers (how deep) and nodes (how wide)), the learning rate (how aggressive the optimizer is),  how long to train, and batch sizes. Finally we want to use the validation data to evaluate the neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOZ2GHkl0nYT",
        "colab_type": "code",
        "outputId": "f142122c-0c00-498c-89a4-4ce903aaee7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "for i in range(0,na_len,1):\n",
        "  print('i = %d'%i)\n",
        "  mse_temp = np.zeros(repeat)\n",
        "  r2_temp = np.zeros(repeat)\n",
        "  for j in range(0,repeat,1):\n",
        "    K.clear_session()\n",
        "    nodes = nodes_array[i]\n",
        "\n",
        "    input_dim_ = len(X_header_list)\n",
        "    output_dim_ = len(Y_header_list)\n",
        "    model = model_neural_network(layer,nodes,input_dim_,output_dim_,DropPercent = droprate)\n",
        "    optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(loss='mean_squared_error',optimizer=\"adam\", metrics=[r2_keras])\n",
        "    history = model.fit(x=X, y = Y, nb_epoch=epochs, batch_size=batchsize,verbose = 0)\n",
        "    Y_pred = model.predict(X_valid)\n",
        "    mse_temp[j] = mse(Y_valid,Y_pred)\n",
        "    r2_temp[j] = r2(Y_valid,Y_pred)\n",
        "    print('j = %d'%j)\n",
        "  mse_mean_array[i] = np.mean(mse_temp)\n",
        "  r2_mean_array[i] = np.mean(r2_temp)\n",
        "  mse_std_array[i] = np.std(mse_temp)\n",
        "  r2_std_array[i] = np.std(r2_temp)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0816 00:54:17.057240 139924467558272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "W0816 00:54:17.061270 139924467558272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0816 00:54:17.101100 139924467558272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0816 00:54:17.102894 139924467558272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0816 00:54:17.110547 139924467558272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0816 00:54:17.132458 139924467558272 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0816 00:54:17.263850 139924467558272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d4f0b148b445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr2_keras\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmse_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 185\u001b[0;31m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZOLFz11Qw0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.errorbar(nodes_array, mse_mean_array, mse_std_array,fmt='o')\n",
        "plt.xlabel('Nodes (n)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "\n",
        "name = str(layer)+'_MSE_Nodes_Study.jpg'\n",
        "plt.savefig(name,dpi=300)\n",
        "files.download(name)\n",
        "\n",
        "plt.figure()\n",
        "plt.errorbar(nodes_array, r2_mean_array, r2_std_array,fmt='o')\n",
        "plt.xlabel('Nodes (n)')\n",
        "plt.ylabel('Linearity ($R^2$)')\n",
        "\n",
        "name = str(layer)+'_R2_Nodes_Study.jpg'\n",
        "plt.savefig(name,dpi=300)\n",
        "files.download(name)\n",
        "\n",
        "print(nodes_array)\n",
        "print(mse_mean_array)\n",
        "print(r2_mean_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGk6PqrLkHeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.vstack([nodes_array,mse_mean_array,mse_std_array,r2_mean_array,r2_std_array]).transpose()\n",
        "name_list = ['Nodes','MSE_Mean','MSE_StD','R2_Mean','R2_StD']\n",
        "name =export_text(data,name_list,layer,repeat)\n",
        "files.download(name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}